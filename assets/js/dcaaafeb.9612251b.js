"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[115],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return m}});var i=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,i,r=function(e,t){if(null==e)return{};var n,i,r={},a=Object.keys(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=i.createContext({}),p=function(e){var t=i.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=p(e.components);return i.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},h=i.forwardRef((function(e,t){var n=e.components,r=e.mdxType,a=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),h=p(n),m=r,d=h["".concat(s,".").concat(m)]||h[m]||u[m]||a;return n?i.createElement(d,o(o({ref:t},c),{},{components:n})):i.createElement(d,o({ref:t},c))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=n.length,o=new Array(a);o[0]=h;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var p=2;p<a;p++)o[p]=n[p];return i.createElement.apply(null,o)}return i.createElement.apply(null,n)}h.displayName="MDXCreateElement"},6045:function(e,t,n){n.r(t),n.d(t,{assets:function(){return c},contentTitle:function(){return s},default:function(){return m},frontMatter:function(){return l},metadata:function(){return p},toc:function(){return u}});var i=n(7462),r=n(3366),a=(n(7294),n(3905)),o=["components"],l={title:"GBest PSO"},s="Global Best PSO (GBestPSO)",p={unversionedId:"usage/gbestpso",id:"usage/gbestpso",title:"GBest PSO",description:"The GBestPSO is the canonical version of the PSO. It is popular, not",source:"@site/../cilib-docs/target/mdoc/usage/gbestpso.md",sourceDirName:"usage",slug:"/usage/gbestpso",permalink:"/docs/usage/gbestpso",draft:!1,tags:[],version:"current",frontMatter:{title:"GBest PSO"}},c={},u=[{value:"Getting things ready",id:"getting-things-ready",level:2}],h={toc:u};function m(e){var t=e.components,n=(0,r.Z)(e,o);return(0,a.kt)("wrapper",(0,i.Z)({},h,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"global-best-pso-gbestpso"},"Global Best PSO (GBestPSO)"),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"GBestPSO")," is the canonical version of the PSO. It is popular, not\nonly, because it is the original version of the algorithm (which is cited\noften within literature), but is also a simple algorithm to implement."),(0,a.kt)("p",null,"As with all algorithms modelled as a function, the type of the ",(0,a.kt)("inlineCode",{parentName:"p"},"GBestPSO"),"\nis simply defined as:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"List[Particle[S,A]] => RVar[List[Particle[S,A]]]\n")),(0,a.kt)("p",null,"where a collection of entities are transformed from a given set of\nentities to a new collection of entities, with randomness applied. This process\nis then repeatedly reapplied, until a stopping condition is reached."),(0,a.kt)("p",null,"We're going to exclude the import statements simply for brevity, but the reader\nis encouraged to examine the example algorithm definition in the ",(0,a.kt)("inlineCode",{parentName:"p"},"examples"),"\nsub-module of the project source."),(0,a.kt)("h2",{id:"getting-things-ready"},"Getting things ready"),(0,a.kt)("p",null,"In order to define an experiment, there are a couple of things we need to\nget ready first. The most obvious should be that there needs to be some kind\nof problem, upon which we will be executing the ",(0,a.kt)("inlineCode",{parentName:"p"},"GBestPSO"),"."),(0,a.kt)("p",null,"As the first step, we need to get the needed imports in scope:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-scala"},"\nmdoc:silent\nimport cilib._\nimport cilib.pso._\nimport cilib.exec._\n\nimport eu.timepit.refined.auto._\n\nimport cilib.syntax.algorithm._\n\n")),(0,a.kt)("p",null,"Next, we define the ",(0,a.kt)("inlineCode",{parentName:"p"},"GBestPSO")," itself. The ",(0,a.kt)("inlineCode",{parentName:"p"},"GBestPSO")," is defined to use a velocity\nupdate equation that uses the personal best of the current particle and then the\ncollection's current best particle to determine the new velocity vector for the\ncurrent particle within the algorithm."),(0,a.kt)("p",null,'Let\'s define the two "particle attractors" which we need in the velocity update\nequation. Because these two values will attract or guide the particle in the search\nspace, we refer to them as ',(0,a.kt)("inlineCode",{parentName:"p"},"Guide")," instances:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-scala"},"val cognitive = Guide.pbest[Mem[Double],Double]\nval social    = Guide.gbest[Mem[Double]]\n")),(0,a.kt)("p",null,"Again, we need to provide some type parameters to keep the compiler happy, but\nin this case we need to provide a type called ",(0,a.kt)("inlineCode",{parentName:"p"},"Mem[Double]"),", which is needed to\ntrack the memory of a particle and at the same time, fulfills the function\nconstraints of the PSO algorithm itself: that the algorithm participants must\ncater for a ",(0,a.kt)("inlineCode",{parentName:"p"},"HasMemory")," instance which exists for the ",(0,a.kt)("inlineCode",{parentName:"p"},"Mem[Double]")," type."),(0,a.kt)("p",null,"Now we can define the algorithm itself, providing some constants that are\nknown to provide convergent behaviour within the PSO:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-scala"},"val gbestPSO = pso.Defaults.gbest(0.729844, 1.496180, 1.496180, cognitive, social)\nval iter = Iteration.sync(gbestPSO)\n")),(0,a.kt)("p",null,'Now that the algorithm is defined, we need to define an "environment"\nwithin which this algorithm will execute. The environment is simply a\ncollection of vaues that defines the comparison and evaluator for the\nalgorithm, such as minimizing a benchmark problem.'),(0,a.kt)("p",null,"Let's define such an environment using a simple problem, borrowing the\nproblem definition from the ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/ciren/benchmarks"},"benchmarks sister\nproject"),". We will also be\nminimizing this problem and defining the bounds of the problem space."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-scala"},"val env =\n  Environment(\n    cmp = Comparison.dominance(Min),\n    eval = Eval.unconstrained(cilib.benchmarks.Benchmarks.spherical[NonEmptyList, Double]))\n\nval bounds = Interval(-5.12,5.12)^30\n")),(0,a.kt)("p",null,"Here we define a the evaluator, which is an unconstrained ",(0,a.kt)("inlineCode",{parentName:"p"},"Eval"),"\ninstance, which uses the ",(0,a.kt)("inlineCode",{parentName:"p"},"spherical")," function definiton from the\nbenchmarks project. We explicitly provide the needed type parameters\nto keep the compiler happy, that being that the ",(0,a.kt)("inlineCode",{parentName:"p"},"Position")," is a\n",(0,a.kt)("inlineCode",{parentName:"p"},"NonEmtpyList[Double]"),". Additionally, the ",(0,a.kt)("inlineCode",{parentName:"p"},"cmp")," value defines ",(0,a.kt)("em",{parentName:"p"},"how"),"\nthe optimization will be driven, which is to minimize the evaluator in\nthis example."),(0,a.kt)("p",null,"Let's now define the entity collection that we need to given the\nalgorithm instance. The collection requires the problem bounds and\nalso defines how the entity instances will be initialized, once random\npositions are generated for the given problem space"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-scala"},"val swarm = Position.createCollection(PSO.createParticle(x => Entity(Mem(x, x.zeroed), x)))(bounds, 20)\n")),(0,a.kt)("p",null,"The last requirement is to provide the RNG instance that will use used within\nthe algorithm. We define this value and then repeatedly run the algorithm\non the entity collection, stopping after 1000 iterations of the algorithm\nhave been performed"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-scala"},"val rng = RNG.fromTime // Seed the RNG with the current time of the computer\n\nval result = Runner.repeat(1000, iter, swarm).provide(env).runAll(rng)\n\nresult._2 match {\n  case -\\/(error) =>\n    // Not much to do. The process failed with an error\n    throw error\n\n  case \\/-(value) =>\n    value.map(x => Lenses._position.get(x))\n}\n")))}m.isMDXComponent=!0}}]);